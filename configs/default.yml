
print_freq: 100
data:
  with_dc: False
  train: 
      root_dir: '/path/to/your/cc12m/'
      meta_file: '/path/to/your/cc12m.csv'

      read_from: dir
      batch_size: 256
      num_workers: 8

      input_size: 224
      test_resize: 256

      image_reader:
          type: pil
      sampler:
          type: distributed_epoch
      transforms:
          type: STANDARD

      ### for entity loss ###
      use_entity: ${model.use_entityloss}
      mask_type: class
      use_distilbert: True
      
      ### for mask loss ###
      cross_image: ${model.use_maskloss}
      
  val:
      type: clip
      read_from: petrel
      batch_size: 64
      num_workers: 8
      pin_memory: False
      input_size: 224
      test_resize: 256
      
      root_dir: '/path/to/your/imagenet/'
      meta_file: 'imagenet_info/val.csv'
      # you can change it to imagenet_info relative path, file already in gitlab

      image_reader:
          type: pil
      sampler:
          type: distributed
      transforms:
          type: ONECROP
      label_texts_ensemble: 'prompt1'
          
  img_aug:
    deit_aug: true
    img_size: 224
    img_scale: [0.08, 1.0]
    interpolation: 'bilinear'
    color_jitter: 0.4
    auto_augment: 'rand-m9-mstd0.5-inc1'
    re_prob: 0.25
    re_mode: 'pixel'
    re_count: 1

  text_aug:
    max_seq_len: 77
    multi_label: 0 # we do not use multi-label contrastive 
    word_type: 'noun'


train:
  start_epoch: 0
  epochs: 30
  warmup_epochs: 2
  base_lr: 1.6e-3
  weight_decay: 0.05
  warmup_lr: 4e-6
  min_lr: 4e-5
  clip_grad: 5.0
  accumulation_steps: 0

  ## amp: O0: not activate, O1: activated ###
  amp_opt_level: O0 
  seed: 0
  use_entity: False

  lr_scheduler:
    name: cosine

  optimizer:
    name: adamw
    eps: 1e-8
    betas: [0.9, 0.999]

evaluate:
  eval_only: false
  eval_freq: 1
  task:
    - cls
    - seg
  cls:
    save_best: true
    template: subset
  seg:
    save_best: true
    cfg: segmentation/configs/_base_/datasets/pascal_voc12.py
    template: simple
    opts: []

checkpoint:
  auto_resume: true
  resume: ''
  stage1_checkpoint: '' ## add this for stage2 training
  freq: 1
  max_kept: -1
  save_freq: 1

model:
  type: MultiLabelContrastive
  img_encoder:
    type: GroupViT
    embed_dim: 768
    num_heads: [8, 8]
    embed_factors: [1, 1]
    depths: [6, 6]
    num_group_tokens: [64, 0]
    num_output_groups: [8]
    drop_rate: 0.0
    drop_path_rate: 0.1
    patch_norm: false
    imgnet_pretrained: 'dino'
    fixed: false
    imgnet_pretrained_checkpoint: '/path/to/your/dino_vitbase16_pretrain.pth'

  text_encoder:
    type: Bert
    context_length: 77
    width: 768
    layers: 6
    vocab_size: 49408
    pretrained: true
    fixed: false

  contrast_temperature: 0.07
  proj_num_layers: 2
  output_dim: 256
  multi_label: ${data.text_aug.multi_label}
  use_entityloss: false
  use_maskloss: false


model_name: '' # display name in the logger
output: ???
tag: default
seed: 0
wandb: false
local_rank: ???
vis: []
### for demo only ####
vocab: ade
image_folder: ???
output_folder: ???